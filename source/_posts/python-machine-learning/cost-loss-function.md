---
title: cost/loss function
copyright: true
date: 2019-07-31 10:44:29
categories:
    - python machine learning
tags:
    - python
    - machine learning
    - cost function
    - 损失函数
---
损失函数的引入和证明。

<!-- more -->

### **1. why cost function**

在 Rosenblatt Perceptron 的训练过程中，输入一次就更新一次模型 θ 。每个
 iris 样本输入进去，就会得到输出，然后更新权重。
样本集共有n个样本，模型会更新n次，再加上外层设置的迭代次数m，模型一共被更新m*n次。然而，最后得到的模型并不一定是最优的结果。虽然可以根据每个模型出错的次数或者可视化的分类结果，人为记录并找到一个接近最优的结果，但是工作量巨大。
于是设想，构造一个关于模型 θ 的函数 f(θ) ，当 f(θ) 收敛时，分类结果是最优的，此时的 θ 就是最优的模型。只要这个函数存在，工作量也就转移到了求函数收敛的问题上。 f(θ) 也就是 cost/loss function 的由来。

### **2. prove existence**

既然需要这样一个函数，那就要证明它的确存在。
Frank Rosenblatt从数学上证明了只要两个类别能够被一个线性超平面分开，则感知机算法一定能够收敛。
```
线性超平面是一个几何体，可以用方程表示，
超平面的自由度比所在n维空间低一个维度，即给定n-1个轴的坐标，可以确定剩下一个轴的坐标。
方程是线性的: 是空间点的各分量的线性组合
方程数量为1
在三维坐标系中，三个轴坐标值的线性组合确定一个超平面即平面
                Ax+By+Cz+D=0，A,B,C,D为常数
在平面直角坐标系中，超平面变成了直线
                Ax+By+C=0
推广到n维空间，就统一叫做超平面。
```
模型就是为了分类，所以从结果可知，如果算法能有效分类，算法自然就会收敛。

### **3. 构造cost function**

上边提到模型的分类算法，它应该具有收敛性。
到目前为止感知机里有一个激活函数，还有一个更新权重的算法。那么这个具有收敛性的算法和二者有什么关系？
cost function又在哪？该怎么构造？
先看激活函数

```
# 激活值

```
​​