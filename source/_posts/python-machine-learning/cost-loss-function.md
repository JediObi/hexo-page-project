---
title: cost/loss function
copyright: true
date: 2019-07-31 10:44:29
categories:
    - python machine learning
tags:
    - python
    - machine learning
    - cost function
    - 损失函数
---
损失函数的引入和证明。

<!-- more -->

### **1. why cost function**

在 Rosenblatt Perceptron 的训练过程中，输入一次就更新一次模型 θ 。每个
 iris 样本输入进去，就会得到输出，然后更新权重。
样本集共有n个样本，模型会更新n次，再加上外层设置的迭代次数m，模型一共被更新m*n次。然而，最后得到的模型并不一定是最优的结果。虽然可以根据每个模型出错的次数或者可视化的分类结果，人为记录并找到一个接近最优的结果，但是工作量巨大。
于是设想，构造一个关于模型 θ 的函数 f(θ) ，当 f(θ) 收敛时，分类结果是最优的，此时的 θ 就是最优的模型。只要这个函数存在，工作量也就转移到了求函数收敛的问题上。 f(θ) 也就是 cost/loss function 的由来。

### **2. prove existence**

既然需要这样一个函数，那就要证明它的确存在。
Frank Rosenblatt从数学上证明了只要两个类别能够被一个线性超平面分开，则感知机算法一定能够收敛。
```
线性超平面是一个几何体，可以用方程表示，
超平面的自由度比所在n维空间低一个维度，即给定n-1个轴的坐标，可以确定剩下一个轴的坐标。
方程是线性的: 是空间点的各分量的线性组合
方程数量为1
在三维坐标系中，三个轴坐标值的线性组合确定一个超平面即平面
                Ax+By+Cz+D=0，A,B,C,D为常数
在平面直角坐标系中，超平面变成了直线
                Ax+By+C=0
推广到n维空间，就统一叫做超平面。
```
模型就是为了分类，所以从结果可知，如果算法能有效分类，算法自然就会收敛。

### **3. 构造cost function**

损失函数是感知机向神经网络发展的过程中提出的。
在感知机算法中，根据神经元（激活函数）的输出来确定各个权重分量的增量，然后更新权重，权重始终朝着结果偏移的负方向改变。
但是感知机并没有一个最优权重的衡量机制，感知机仅仅是有自学习的特性，每次学习都是盲目的，并不是朝着最优的结果在学习，单个样本最优化并不能代表一个整体。
于是不再每个样本更新，而是一个样本集计算完根据结果更新一次，每次输出不再更新权重只记录结果（或结果与真实的差值），计算整个样本集结果差值的和`s`，s的最小化过程就是模型在该样本集优化的过程。这样训练出的模型并不一定是单个样本的最优，但它是整个训练集的最优，可以代表一个集合。
而损失函数就是计算`s`的函数。 

### **4. 构造函数的出现对激活函数的影响**

在感知机算法中，激活函数是阶跃函数，它的一个特点是结果单一（收敛），但是缺点是不够连续，很多概率类问题并不是非黑即白的结果。所以引入了具有连续性但又类似阶跃函数的收敛函数，比如logistic函数。​​