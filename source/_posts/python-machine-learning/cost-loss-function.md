---
title: cost/loss function
copyright: true
date: 2019-07-31 10:44:29
categories:
    - python machine learning
tags:
    - python
    - machine learning
    - cost function
    - 损失函数
---
损失函数的引入和证明。

<!-- more -->

### **1. why cost function**

在 Rosenblatt Perceptron 的训练过程中，输入一次就更新一次模型 θ 。每个
 iris 样本输入进去，就会得到输出，然后更新权重。
样本集共有n个样本，模型会更新n次，再加上外层设置的迭代次数m，模型一共被更新m*n次。然而，最后得到的模型并不一定是最优的结果。虽然可以根据每个模型出错的次数或者可视化的分类结果，人为记录并找到一个接近最优的结果，但是工作量巨大。
于是设想，构造一个关于模型 θ 的函数 f(θ) ，当 f(θ) 收敛时，分类结果是最优的，此时的 θ 就是最优的模型。只要这个函数存在，工作量也就转移到了求函数收敛的问题上。 f(θ) 也就是 cost/loss function 的由来。

### **2. prove existence**

既然需要这样一个函数，那就要证明它的确存在。
Frank Rosenblatt从数学上证明了只要两个类别能够被一个线性超平面分开，则感知机算法一定能够收敛。
```
线性超平面是一个几何体，可以用方程表示，
超平面的自由度比所在n维空间低一个维度，即给定n-1个轴的坐标，可以确定剩下一个轴的坐标。
方程是线性的: 是空间点的各分量的线性组合
方程数量为1
在三维坐标系中，三个轴坐标值的线性组合确定一个超平面即平面
                Ax+By+Cz+D=0，A,B,C,D为常数
在平面直角坐标系中，超平面变成了直线
                Ax+By+C=0
推广到n维空间，就统一叫做超平面。
```
模型就是为了分类，所以从结果可知，如果算法能有效分类，算法自然就会收敛。

### **3. cost function定义**

接下来看损失函数的定义

如何量化一个模型的好坏，在单个样本上是结果与真实值越小越好，在整个样本集上也是这样（可以是对单个样本差值求和的结果）。

求样本集结果差值的函数就是损失函数。损失函数的选择很重要。

到目前为止，已经产生了 拟合函数 激活函数 损失函数。损失函数的定义可以看出它的确是一个关于模型(拟合函数)的函数。

综上，对损失函数的求收敛就对应最优化模型

### **4. 构造函数的出现对激活函数的影响**

在感知机算法中，激活函数是阶跃函数，它的一个特点是结果单一（收敛），但是缺点是不够连续，很多概率类问题并不是非黑即白的结果。所以引入了具有连续性但又类似阶跃函数的收敛函数，比如logistic函数。​​