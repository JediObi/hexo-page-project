---
title: 从Perceptron到Logistic
copyright: true
date: 2019-07-31 10:41:49
categories:
    - python machine learning
tags:
    - python
    - machine learning
---
激活函数的变化，从阶跃函数到logistic。

<!-- more -->

### **1. 最开始学习的单细胞模型——感知机**

（1）构建的线性超平面直接作为激活值
（2）每次输入一个样本更新一次模型
（3）权重增量 (y_实 - y_预)

### **2. 然后是Adaline**

（1）一个样本集更新一次模型，因此定义了一个训练集的损失函数
（2）使用差平方和作为构成损失函数
（3）权重增量使用（- 学习率 * 损失函数增量），所以要对损失函数求偏导来求权重增量；而随着权重的变化，损失函数收敛时，即得到最优模型。使用梯度下降寻找收敛，同时就能的得到最优模型
（4）使用梯度下降求收敛，而不使用驻点，因为导数的解析解不好求，并且导数求出的解不一定是方程的解。
（5）使用随机梯度下降，快速收敛。具体就是每输入一个样本（作者：可能是说整个训练样本集）就更新一次模型。也可以把训练集拆分成更小的子集，每个子集更新一次模型。

### **3. 现在是Logistic**

（1）使用sigmoid这样的激活函数，函数图像是连续可导的。取代了之前的阶跃函数
（2）激活函数求出的是一个预测概率且是一个后验概率p，关于使用后验概率的事以后证明。此处定义了一个logit(p)=log(p/(1-p))=z, 与激活函数互为反函数。p=phi(z)
（3）在后验概率基础上构造出预测结果的表达式——可以看做是单个样本的似然函数，以此构造出整个训练集的似然函数——对预测结果求和，然后使用最大似然估计法求出似然函数最大值，它对应最大模型。怎么求最大似然，由于变量和它的对数函数单调性相同，那么似然函数的的最大值必然对应它的负对数的收敛。所以使用似然函数的负对数求收敛。而这个负对数就是损失函数——J(w)
（4）为什么使用最大似然。可以从单个样本来看，由于sigmoid函数值(0,1)。
根据单个似然函数的表达式定义可以有如下解释
如果label=1，J(w) = -log(phi)
预测正确时sigmoid趋近于1， -log(sigmoid)=-log(sigmoid=>1)=>0，收敛。
预测错误时sigmoid趋近0， -log(sigmoid)=-log(sigmoid=>0)=>正无穷。
如果label=0， J(w) = -log(1-phi)
预测正确时sigmoid趋近0， -log(1-(sigmoid=>0))=>-log(1)=>0，收敛。
预测错误时sigmoid趋近1，-log(1-(sigmoid=>1))=>-log(0)=>正无穷。
综上，如果预测都趋向于正确，则损失函数收敛，那么就能找到最优模型。
