---
title: 常用的损失函数
copyright: true
date: 2019-08-08 11:33:38
categories:
tags:
---
文章简介

<!-- more -->

交叉熵 有多种交叉熵

什么是交叉熵

事件发生的概率 p(x)

事件信息量， 越可能发生的事件发生了，其信息量越小； 越不可能发生的事件发生了，其信息量越大。
I(x) = -log(p(x)) 概率的负对数， 与概率成负相关。

熵， 事件信息量的期望，(对应于统计学算数平均值，也可以是加权平均值，到概率论就是事件与发生概率的乘积的和)
H = -Ep(x)log(p(x))

相对熵(KL散度)
用于衡量同一个随机变量两种分布的差异， 比如p(x) x的真实分布， q(x) 预测的x的分布概率

Ep(x)log(p(x)/q(x)), 由这个公式可知 值越小二者越接近

交叉熵

把KL散度拆开

Ep(x)log(p(x)/q(x)) = Ep(x)log(p(x)) - Ep(x)log(q(x)) = -H(p) + H(p,q)

H(p) 就是真实分布的熵，是一个统计值（基于已有数据计算得来），不变的值; H(p, q)，就是交叉熵，所以这部分越小，就表示q与真实分布越接近。通常可以把交叉熵用做分类模型损失函数。 而回归则使用mse一类的损失函数


(2) 交叉熵

交叉熵会让分类错误的一结果消失， 分类正确的结果增大。

(3) mse

在sigmoid激活函数里，函数的两端偏导数接近0，导致w,b的梯度消失

交叉熵不会。

所以交叉熵更适合离散数据，分类模型。而对于连续的求值型使用mse这类合适.