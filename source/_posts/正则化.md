---
title: 正则化
copyright: true
date: 2019-08-09 16:01:10
categories:
tags:
---
文章简介

<!-- more -->

正则化用于防止模型过拟合。

过拟合直观上就是模型过多的学习了训练集上的特征，使得超线性方程能够覆盖每个数据点。
这通常是因为模型太复杂，学习到了无用的特征。模型的复杂度对应了它的参数分布的熵（也可以理解为参数的分布有序程度），熵大则复杂，熵小则简单。通过熵公式可知，降低熵可以通过减少参数，或通过降低参数的值来达到目的。
过拟合，模型在很小的区间上就会出现剧烈的抖动，在数学上表现为导数很大。而导数很大通常是因为系数很大，也就是模型的权重过大。权重过大也符合熵增的特点。
通常不会减少参数数量，因为模型学习的潜力也会下降。所以就通过降低参数值来降低复杂度， 这就是参数正则化。

正则化通过给损失函数(目标函数)增加一个正则项来防止过拟合。

正则项有两种， l1和l2

l1正则使用权重绝对值的和构造 (λ/2n)E(|w|), λ是正则化参数，是一个学习参数

l2又称为权重衰减，使用权重平方和构造, (λ/2n)E(w^2)

假设原始的损失函数C0，正则项为r，新的损失函数就是
C = C0 + r

梯度下降求收敛和优化模型的过程变化

回顾下模型的优化过程，每次预测结果出来，根据结果反推出一个权重增量d(w)， 如果结果小于真实值则让权重增加一点，如果结果偏大则让权重减小一点。

w = w - αd(w)，α是最初的学习率。

对于增量的求解的变化， 在鸢尾花中的例子里,预测值反推出模型，反推出的结果和已有的模型差值作为增量， 而到adaline中训练的方式从单个数据变成了一个训练集，并且产生了损失函数，损失函数是一个关于模型的函数，而损失函数的优化等价于模型的优化，使用梯度下降来优化损失函数。

梯度下降的原理是，在梯度方向上使权重变化一个极小的值，逐步使损失函数收敛。而梯度下降的优势是下降速度快。

在加入正则项之后，梯度下降的微分变成 d(C)/d(w) = d(C0)/d(w) + d(r)/d(w)

l2正则化损失函数的微分

 = d(C0)/d(w) + (λ/n)w，λ作为超参，通常使用固定值，在训练过程中根据需要改变，就像学习率，在开始比较大，在结束时比较小，
 梯度下降就变成了 w-η(d(C0)/d(w) + (λ/n)w) = (1-(ηλ/n))w-ηd(C0)/d(w)， λ过大就导致一个梯度下降之后权重迅速下降趋近于0，模型复杂度迅速下降，而那些错误的特征权重会快速逼近0。

 l1正则项因为符合拉普拉斯分布，最优值通常出现在坐标轴上，造成某些维度的权重为0， 产生权重的稀疏矩阵，降低复杂度，避免学习无用的特征防止过拟合。

 l2正则项符合高斯分布，最后也是降低参数值防止过拟合。